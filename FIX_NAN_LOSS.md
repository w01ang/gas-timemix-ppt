# ä¿®å¤ NaN æŸå¤±é—®é¢˜æŒ‡å—

è®­ç»ƒæ—¶éªŒè¯æŸå¤±å˜æˆNaNæ˜¯æ¢¯åº¦çˆ†ç‚¸æˆ–æ•°å€¼ä¸ç¨³å®šçš„æ ‡å¿—ã€‚

---

## ğŸ” è¯Šæ–­æµç¨‹

### 1. ç¡®å®šNaNå‡ºç°æ—¶æœº

**åœ¨ç¬¬ä¸€ä¸ªepochå°±NaNï¼Ÿ**
- âœ… æ˜¯ â†’ å­¦ä¹ ç‡è¿‡é«˜æˆ–æ•°æ®é—®é¢˜
- âŒ å¦ â†’ æ¢¯åº¦çˆ†ç‚¸æˆ–æ•°å€¼ä¸ç¨³å®š

**æŸå¤±å€¼å˜åŒ–è¶‹åŠ¿ï¼š**
```
æ­£å¸¸: 1.5 â†’ 1.2 â†’ 1.0 â†’ 0.8 â†’ ...
å¼‚å¸¸: 1.5 â†’ 3.2 â†’ 10.5 â†’ Inf â†’ NaN  â† æ¢¯åº¦çˆ†ç‚¸
å¼‚å¸¸: 1.5 â†’ 1.2 â†’ NaN                â† çªç„¶å´©æºƒ
```

---

## ğŸš¨ ç«‹å³è§£å†³æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### æ–¹æ¡ˆ1ï¼šå¤§å¹…é™ä½å­¦ä¹ ç‡ â­â­â­â­â­

**æœ€å¸¸è§çš„åŸå› å’Œæœ€å¿«çš„è§£å†³æ–¹æ³•ï¼**

```bash
# å¦‚æœå½“å‰ä½¿ç”¨ --learning_rate 1e-4 æˆ–æ›´é«˜
# ç«‹å³é™ä½åˆ°:

python scripts/train_sliding_window.py \
  --model_id slide_fix_nan_lr1e5 \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 16 \
  --learning_rate 1e-5 \
  --train_epochs 60 \
  --use_gpu
```

**å¦‚æœè¿˜æ˜¯NaNï¼Œç»§ç»­é™ä½ï¼š**
```bash
--learning_rate 5e-6  # æ›´ä¿å®ˆ
```

---

### æ–¹æ¡ˆ2ï¼šæ·»åŠ æ¢¯åº¦è£å‰ª â­â­â­â­â­

ä¿®æ”¹è®­ç»ƒè„šæœ¬ï¼Œæ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼š

```python
# åœ¨ train_sliding_window.py çš„è®­ç»ƒå¾ªç¯ä¸­
# æ‰¾åˆ° loss.backward() ä¹‹åï¼Œæ·»åŠ ï¼š

import torch.nn as nn

# loss.backward() ä¹‹åæ·»åŠ 
nn.utils.clip_grad_norm_(exp.model.parameters(), max_norm=1.0)

model_optim.step()
```

**å®Œæ•´ä¿®æ”¹ç¤ºä¾‹ï¼š**

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­ (å¤§çº¦ç¬¬160è¡Œé™„è¿‘)
loss.backward()

# ã€æ·»åŠ è¿™ä¸€è¡Œã€‘æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(exp.model.parameters(), max_norm=1.0)

model_optim.step()
```

---

### æ–¹æ¡ˆ3ï¼šæ£€æŸ¥å¹¶æ¸…ç†æ•°æ® â­â­â­â­

```python
# åˆ›å»ºæ•°æ®æ£€æŸ¥è„šæœ¬
import pandas as pd
import numpy as np

# è¯»å–æ•°æ®
data_path = '/Users/wangjr/Documents/yk/timemixer/data/preprocessed_daily_gas_by_well.csv'
df = pd.read_csv(data_path)

print("æ•°æ®å¼‚å¸¸å€¼æ£€æŸ¥:")
print("-" * 50)

# æ£€æŸ¥NaN
nan_count = df['OT'].isna().sum()
print(f"NaNå€¼æ•°é‡: {nan_count}")

# æ£€æŸ¥Inf
inf_count = np.isinf(df['OT']).sum()
print(f"Infå€¼æ•°é‡: {inf_count}")

# æ£€æŸ¥å¼‚å¸¸å¤§çš„å€¼
max_val = df['OT'].max()
min_val = df['OT'].min()
mean_val = df['OT'].mean()
std_val = df['OT'].std()

print(f"\næ•°æ®ç»Ÿè®¡:")
print(f"  æœ€å¤§å€¼: {max_val}")
print(f"  æœ€å°å€¼: {min_val}")
print(f"  å‡å€¼: {mean_val:.2f}")
print(f"  æ ‡å‡†å·®: {std_val:.2f}")

# æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„å€¼
threshold = mean_val + 10 * std_val
outliers = (df['OT'] > threshold).sum()
print(f"\nè¶…è¿‡å‡å€¼+10å€æ ‡å‡†å·®çš„å¼‚å¸¸å€¼: {outliers}")

# å¦‚æœå‘ç°å¼‚å¸¸ï¼Œæ¸…ç†æ•°æ®
if nan_count > 0 or inf_count > 0 or outliers > 100:
    print("\nâš ï¸  å‘ç°å¼‚å¸¸æ•°æ®ï¼å»ºè®®æ¸…ç†ã€‚")
    
    # æ¸…ç†æ–¹æ¡ˆ
    df_clean = df.copy()
    
    # 1. åˆ é™¤NaNå’ŒInf
    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)
    df_clean = df_clean.dropna()
    
    # 2. è£å‰ªå¼‚å¸¸å€¼
    upper_bound = mean_val + 5 * std_val
    lower_bound = max(0, mean_val - 5 * std_val)  # äº§é‡ä¸èƒ½ä¸ºè´Ÿ
    df_clean['OT'] = df_clean['OT'].clip(lower_bound, upper_bound)
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    output_path = '/Users/wangjr/Documents/yk/timemixer/data/preprocessed_daily_gas_by_well_cleaned.csv'
    df_clean.to_csv(output_path, index=False)
    print(f"\nâœ… æ¸…ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print(f"   åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"   æ¸…ç†åè¡Œæ•°: {len(df_clean)}")
else:
    print("\nâœ… æ•°æ®æ­£å¸¸ï¼Œæ— å¼‚å¸¸å€¼")
```

ä¿å­˜ä¸º `scripts/check_data.py` å¹¶è¿è¡Œï¼š
```bash
cd /Users/wangjr/Documents/yk/timemixer/timemixer-ppt/gas-timemix
python scripts/check_data.py
```

---

### æ–¹æ¡ˆ4ï¼šä½¿ç”¨æ›´ç¨³å®šçš„æŸå¤±å‡½æ•° â­â­â­

ä»MAEæ”¹ä¸ºHuber Lossï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰ï¼š

```python
# åœ¨ exp/exp_long_term_forecasting.py çš„ _select_criterion ä¸­
def _select_criterion(self):
    if self.args.loss == 'Huber':
        criterion = nn.HuberLoss(delta=1.0)  # æ·»åŠ Huber Loss
    elif self.args.loss == 'MSE':
        criterion = nn.MSELoss()
    elif self.args.loss == 'MAE':
        criterion = nn.L1Loss()
    else:
        criterion = nn.L1Loss()
    return criterion
```

ç„¶åè®­ç»ƒæ—¶ä½¿ç”¨ï¼š
```bash
python scripts/train_sliding_window.py \
  --model_id slide_huber_loss \
  --loss Huber \
  --learning_rate 5e-5 \
  ...
```

---

### æ–¹æ¡ˆ5ï¼šé™ä½æ¨¡å‹å¤æ‚åº¦ â­â­â­

å¦‚æœä»¥ä¸Šéƒ½ä¸è¡Œï¼Œå¯èƒ½æ˜¯æ¨¡å‹å¤ªå¤æ‚å¯¼è‡´ä¸ç¨³å®šï¼š

```bash
python scripts/train_sliding_window.py \
  --model_id slide_simple_stable \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 32 \
  --d_model 64 \
  --n_heads 4 \
  --e_layers 2 \
  --d_layers 1 \
  --d_ff 256 \
  --dropout 0.2 \
  --learning_rate 5e-5 \
  --train_epochs 60 \
  --use_gpu
```

---

## ğŸ› ï¸ å®Œæ•´ä¿®å¤æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿä¿®å¤ï¼ˆ2åˆ†é’Ÿï¼‰

```bash
# ç«‹å³ç”¨æ›´ä½çš„å­¦ä¹ ç‡é‡æ–°è®­ç»ƒ
python scripts/train_sliding_window.py \
  --model_id slide_fix_nan \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 16 \
  --learning_rate 1e-5 \
  --train_epochs 60 \
  --use_gpu
```

### ç¬¬äºŒæ­¥ï¼šæ·»åŠ æ¢¯åº¦è£å‰ªï¼ˆ5åˆ†é’Ÿï¼‰

ä¿®æ”¹ `scripts/train_sliding_window.py`ï¼š

```python
# æ‰¾åˆ°ç¬¬160è¡Œå·¦å³çš„è®­ç»ƒå¾ªç¯
# åœ¨ loss.backward() ä¹‹åæ·»åŠ ï¼š

loss.backward()
torch.nn.utils.clip_grad_norm_(exp.model.parameters(), max_norm=1.0)  # ã€æ–°å¢ã€‘
model_optim.step()
```

### ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ•°æ®ï¼ˆ5åˆ†é’Ÿï¼‰

åˆ›å»ºå¹¶è¿è¡Œæ•°æ®æ£€æŸ¥è„šæœ¬ï¼ˆè§æ–¹æ¡ˆ3ï¼‰

### ç¬¬å››æ­¥ï¼šé‡æ–°è®­ç»ƒï¼ˆ30åˆ†é’Ÿï¼‰

ä½¿ç”¨ä¿®æ”¹åçš„è„šæœ¬å’Œæ¸…ç†åçš„æ•°æ®é‡æ–°è®­ç»ƒ

---

## ğŸ“Š é¢„é˜²æªæ–½

### è®­ç»ƒæ—¶ç›‘æ§æŒ‡æ ‡

æ·»åŠ åˆ°è®­ç»ƒå¾ªç¯ä¸­ï¼š

```python
# åœ¨æ¯ä¸ªbatchåæ£€æŸ¥
if torch.isnan(loss):
    print(f"âš ï¸  æ£€æµ‹åˆ°NaN! Batch {i}, Loss: {loss.item()}")
    print(f"   å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]}")
    # ä¿å­˜å‡ºé—®é¢˜çš„batchæ•°æ®
    torch.save({
        'batch_x': batch_x,
        'batch_y': batch_y,
        'model_state': exp.model.state_dict(),
    }, 'nan_debug.pth')
    raise ValueError("è®­ç»ƒå‡ºç°NaNï¼Œå·²ä¿å­˜è°ƒè¯•ä¿¡æ¯")

# ç›‘æ§æ¢¯åº¦
if i % 10 == 0:
    total_norm = 0
    for p in exp.model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    if total_norm > 10.0:
        print(f"âš ï¸  è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å¤§ {total_norm:.2f}")
```

### æ¨èçš„ç¨³å®šè®­ç»ƒé…ç½®

```bash
python scripts/train_sliding_window.py \
  --model_id slide_ultra_stable \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 32 \
  --d_model 128 \
  --n_heads 8 \
  --e_layers 3 \
  --d_layers 1 \
  --d_ff 512 \
  --dropout 0.2 \
  --learning_rate 5e-5 \
  --patience 25 \
  --train_epochs 80 \
  --use_gpu
```

**å…³é”®é…ç½®ç‚¹ï¼š**
- âœ… å­¦ä¹ ç‡ï¼š5e-5ï¼ˆä¿å®ˆï¼‰
- âœ… Batch sizeï¼š32ï¼ˆæ›´ç¨³å®šï¼‰
- âœ… æ¨¡å‹ä¸­ç­‰å¤§å°ï¼ˆd_model=128ï¼‰
- âœ… Dropoutï¼š0.2ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰

---

## ğŸ”¬ è°ƒè¯•æŠ€å·§

### 1. æ‰¾å‡ºå“ªä¸ªepochå‡ºç°NaN

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -20 logs/YOUR_MODEL_ID/training_summary.txt
```

### 2. æ£€æŸ¥æ¨¡å‹æƒé‡

```python
import torch

checkpoint = torch.load('checkpoints/YOUR_MODEL_ID/checkpoint.pth')
for key, value in checkpoint.items():
    if torch.isnan(value).any():
        print(f"å‘ç°NaNæƒé‡: {key}")
    if torch.isinf(value).any():
        print(f"å‘ç°Infæƒé‡: {key}")
```

### 3. é€æ­¥é™ä½å­¦ä¹ ç‡æµ‹è¯•

```bash
# æµ‹è¯•å“ªä¸ªå­¦ä¹ ç‡æ˜¯å®‰å…¨çš„
for lr in 1e-4 5e-5 1e-5 5e-6; do
  echo "æµ‹è¯•å­¦ä¹ ç‡: ${lr}"
  python scripts/train_sliding_window.py \
    --model_id test_lr_${lr} \
    --learning_rate ${lr} \
    --train_epochs 5 \
    --use_gpu
  
  # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
  if [ $? -eq 0 ]; then
    echo "âœ… å­¦ä¹ ç‡ ${lr} å¯ç”¨"
  else
    echo "âŒ å­¦ä¹ ç‡ ${lr} å¯¼è‡´NaN"
  fi
done
```

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå­¦ä¹ ç‡1e-4ä¼šå¯¼è‡´NaNï¼Ÿ
A: OneCycleLRä¼šåœ¨è®­ç»ƒä¸­æœŸå°†å­¦ä¹ ç‡æå‡åˆ°å³°å€¼ï¼ˆçº¦2-3å€ï¼‰ï¼Œæ‰€ä»¥å®é™…æœ€é«˜å­¦ä¹ ç‡å¯èƒ½è¾¾åˆ°2e-4æˆ–3e-4ï¼Œå¯¹äºå¤§æ¨¡å‹å¤ªé«˜äº†ã€‚

### Q2: æ¢¯åº¦è£å‰ªçš„max_normè¯¥è®¾å¤šå°‘ï¼Ÿ
A: 
- 1.0ï¼šéå¸¸ä¿å®ˆï¼Œé€‚åˆä¸ç¨³å®šçš„è®­ç»ƒ
- 5.0ï¼šä¸­ç­‰ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ
- 10.0ï¼šå®½æ¾ï¼Œé€‚åˆç¨³å®šçš„è®­ç»ƒ

### Q3: ä¿®æ”¹åéœ€è¦é‡æ–°è®­ç»ƒå—ï¼Ÿ
A: æ˜¯çš„ï¼Œä¸€æ—¦å‡ºç°NaNï¼Œæ¨¡å‹æƒé‡å·²æŸåï¼Œå¿…é¡»é‡æ–°å¼€å§‹ã€‚

### Q4: å¦‚ä½•çŸ¥é“æ˜¯å“ªä¸€å±‚å¯¼è‡´çš„NaNï¼Ÿ
A: åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼š
```python
for name, param in exp.model.named_parameters():
    if param.grad is not None and torch.isnan(param.grad).any():
        print(f"NaNæ¢¯åº¦åœ¨: {name}")
```

---

## ğŸ¯ æ¨èè¡ŒåŠ¨æ–¹æ¡ˆ

**å¦‚æœä½ çš„æ¨¡å‹åˆšå¼€å§‹å°±NaNï¼š**

1. ç«‹å³é™ä½å­¦ä¹ ç‡åˆ° **1e-5**
2. æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰å¼‚å¸¸å€¼
3. ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆd_model=64ï¼‰

**å¦‚æœè®­ç»ƒåˆ°ä¸­é€”æ‰NaNï¼š**

1. æ·»åŠ æ¢¯åº¦è£å‰ªï¼ˆmax_norm=1.0ï¼‰
2. é™ä½å­¦ä¹ ç‡åˆ° **5e-5**
3. å¢åŠ batch sizeåˆ°32

**å¦‚æœä»¥ä¸Šéƒ½ä¸è¡Œï¼š**

1. ä½¿ç”¨Huber Lossæ›¿ä»£MAE
2. å¤§å¹…ç®€åŒ–æ¨¡å‹ï¼ˆd_model=32ï¼‰
3. æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®é—®é¢˜

---

## âœ… éªŒè¯ä¿®å¤æ˜¯å¦æˆåŠŸ

è®­ç»ƒå¼€å§‹åï¼Œè§‚å¯Ÿå‰10ä¸ªepochï¼š

```
Epoch 1: Train=1.5, Vali=1.3, Test=1.4  âœ…
Epoch 2: Train=1.2, Vali=1.1, Test=1.2  âœ…
Epoch 3: Train=1.0, Vali=0.9, Test=1.0  âœ…
...

å¦‚æœçœ‹åˆ°å¹³æ»‘ä¸‹é™ï¼Œè¯´æ˜ä¿®å¤æˆåŠŸï¼
```

**ä¸æ­£å¸¸çš„æƒ…å†µï¼š**
```
Epoch 1: Train=5.2, Vali=3.8  âš ï¸  åˆå§‹æŸå¤±è¿‡é«˜
Epoch 2: Train=NaN            âŒ  ç«‹å³å´©æºƒ
```

---

**ç«‹å³å°è¯•æ–¹æ¡ˆ1ï¼ˆé™ä½å­¦ä¹ ç‡ï¼‰ï¼è¿™è§£å†³äº†90%çš„NaNé—®é¢˜ï¼** ğŸš€

