# TimeMixer æ·±åº¦æ”¹è¿›æ–¹æ³•æŒ‡å—

æ ¹æ®å½“å‰4ä¸ªç‰ˆæœ¬çš„è®­ç»ƒç»“æœï¼Œæä¾›ç³»ç»Ÿæ€§æ”¹è¿›æ–¹æ¡ˆã€‚

---

## ğŸ“Š å½“å‰æœ€ä½³åŸºçº¿

**v2 (step50_bs16)** æ˜¯ç›®å‰è¡¨ç°æœ€å¥½çš„ç‰ˆæœ¬ï¼š
- æµ‹è¯•æŸå¤±ï¼š**0.390** âœ…
- Epoch 17æ—¶è¾¾åˆ°æœ€ä½³
- ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆd_model=256, n_heads=16, e_layers=6ï¼‰
- ä»æœ‰è¿‡æ‹Ÿåˆï¼ˆå·®è·0.236ï¼‰

---

## ğŸš€ æ”¹è¿›æ–¹å‘ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### 1ï¸âƒ£ ä¼˜åŒ–ç°æœ‰æœ€ä½³æ¨¡å‹ï¼ˆâ­â­â­â­â­ æœ€æ¨èï¼‰

**ç­–ç•¥ï¼šåœ¨v2åŸºç¡€ä¸Šå¾®è°ƒï¼Œè€Œä¸æ˜¯å¤§å¹…é™ä½æ¨¡å‹å®¹é‡**

#### æ–¹æ¡ˆAï¼šè½»å¾®é™ä½å¤æ‚åº¦ + å¢å¼ºæ­£åˆ™åŒ–

```bash
python scripts/train_sliding_window.py \
  --model_id slide_v2_optimized \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 16 \
  --d_model 192 \
  --n_heads 12 \
  --e_layers 4 \
  --d_layers 2 \
  --d_ff 768 \
  --dropout 0.15 \
  --learning_rate 8e-5 \
  --patience 20 \
  --train_epochs 80 \
  --use_gpu
```

**æ”¹è¿›ç‚¹ï¼š**
- ä¿æŒè¾ƒå¤§æ¨¡å‹å®¹é‡ï¼ˆä»256â†’192ï¼Œä»…é™ä½25%ï¼‰
- è½»å¾®å¢åŠ dropoutï¼ˆ0.1â†’0.15ï¼‰
- ä¿æŒv2çš„step_len=50å’Œbatch_size=16
- é¢„æœŸæµ‹è¯•æŸå¤±ï¼š**0.35-0.37**

#### æ–¹æ¡ˆBï¼šv2 + L2æ­£åˆ™åŒ–

éœ€è¦ä¿®æ”¹è®­ç»ƒè„šæœ¬æ·»åŠ weight decayï¼š

```python
# åœ¨ exp/exp_long_term_forecasting.py çš„ _select_optimizer ä¸­
model_optim = optim.Adam(
    self.model.parameters(),
    lr=self.args.learning_rate,
    weight_decay=1e-5  # æ·»åŠ L2æ­£åˆ™åŒ–
)
```

ç„¶åè¿è¡Œï¼š
```bash
python scripts/train_sliding_window.py \
  --model_id slide_v2_l2reg \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 16 \
  --dropout 0.15 \
  --train_epochs 80 \
  --use_gpu
```

---

### 2ï¸âƒ£ æ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆâ­â­â­â­â­ï¼‰

#### A. å™ªå£°æ³¨å…¥

åœ¨æ•°æ®åŠ è½½æ—¶æ·»åŠ å°é‡å™ªå£°ï¼š

```python
# åœ¨ data_provider/data_loader.py çš„ __getitem__ ä¸­
if self.flag == 'train':
    noise = np.random.normal(0, 0.01, seq_x.shape)
    seq_x = seq_x + noise
```

#### B. æ—¶é—´çª—å£æŠ–åŠ¨

éšæœºè°ƒæ•´æ»‘åŠ¨çª—å£çš„èµ·å§‹ä½ç½®ï¼š

```python
# åœ¨ç”Ÿæˆæ ·æœ¬æ—¶
if self.flag == 'train':
    jitter = np.random.randint(-10, 10)  # Â±10æ­¥æŠ–åŠ¨
    start_idx = max(0, start_idx + jitter)
```

#### C. Mixupå¢å¼º

æ··åˆä¸åŒäº•çš„æ•°æ®ï¼š

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
if np.random.rand() < 0.3:  # 30%æ¦‚ç‡
    lambda_mix = np.random.beta(0.2, 0.2)
    idx = torch.randperm(batch_x.size(0))
    batch_x = lambda_mix * batch_x + (1 - lambda_mix) * batch_x[idx]
    batch_y = lambda_mix * batch_y + (1 - lambda_mix) * batch_y[idx]
```

---

### 3ï¸âƒ£ æ”¹è¿›å­¦ä¹ ç‡ç­–ç•¥ï¼ˆâ­â­â­â­ï¼‰

#### A. ä½¿ç”¨CosineAnnealingæ›¿ä»£OneCycleLR

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

scheduler = CosineAnnealingWarmRestarts(
    model_optim,
    T_0=10,  # æ¯10ä¸ªepoché‡å¯
    T_mult=2,  # å‘¨æœŸå€å¢
    eta_min=1e-6
)
```

#### B. åˆ†æ®µå­¦ä¹ ç‡

```bash
# ç¬¬ä¸€é˜¶æ®µï¼šé«˜å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
python scripts/train_sliding_window.py \
  --model_id slide_stage1 \
  --learning_rate 1e-4 \
  --train_epochs 20

# ç¬¬äºŒé˜¶æ®µï¼šä½å­¦ä¹ ç‡ç²¾ç»†è°ƒä¼˜
# åŠ è½½stage1çš„æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
python scripts/train_sliding_window.py \
  --model_id slide_stage2 \
  --learning_rate 2e-5 \
  --train_epochs 40 \
  --load_checkpoint checkpoints/slide_stage1/checkpoint.pth
```

---

### 4ï¸âƒ£ æ¨¡å‹æ¶æ„æ”¹è¿›ï¼ˆâ­â­â­â­ï¼‰

#### A. æ·»åŠ æ®‹å·®è¿æ¥å¢å¼º

åœ¨TimeMixeråŸºç¡€ä¸Šæ·»åŠ æ›´å¤šskip connections

#### B. ä½¿ç”¨ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶

å°è¯•ä»¥ä¸‹å˜ä½“ï¼š
- Flash Attentionï¼ˆæ›´é«˜æ•ˆï¼‰
- Linear Attentionï¼ˆé™ä½å¤æ‚åº¦ï¼‰
- Local Attentionï¼ˆåªå…³æ³¨å±€éƒ¨çª—å£ï¼‰

#### C. å±‚æ¬¡åŒ–é¢„æµ‹

å…ˆé¢„æµ‹ç²—ç²’åº¦è¶‹åŠ¿ï¼Œå†é¢„æµ‹ç»†èŠ‚ï¼š

```
Encoder â†’ Coarse Prediction (100æ­¥)
       â†’ Fine Prediction (1000æ­¥åŸºäºç²—é¢„æµ‹)
```

---

### 5ï¸âƒ£ é›†æˆå­¦ä¹ ï¼ˆâ­â­â­â­ï¼‰

#### A. å¤šæ¨¡å‹é›†æˆ

è®­ç»ƒ5ä¸ªä¸åŒåˆå§‹åŒ–çš„æ¨¡å‹ï¼š

```bash
for seed in 1 2 3 4 5; do
  python scripts/train_sliding_window.py \
    --model_id slide_ensemble_seed${seed} \
    --input_len 3000 \
    --output_len 1000 \
    --step_len 50 \
    --batch_size 16 \
    --train_epochs 60 \
    --use_gpu
done
```

é¢„æµ‹æ—¶å–å¹³å‡ï¼š
```python
predictions = []
for seed in range(1, 6):
    model = load_model(f'slide_ensemble_seed{seed}')
    pred = model.predict(x)
    predictions.append(pred)

final_pred = np.mean(predictions, axis=0)
```

**é¢„æœŸæå‡ï¼š5-10%**

#### B. ä¸åŒé…ç½®é›†æˆ

```bash
# æ¨¡å‹1ï¼šå¤§æ¨¡å‹
--d_model 256 --e_layers 6

# æ¨¡å‹2ï¼šä¸­æ¨¡å‹
--d_model 128 --e_layers 4

# æ¨¡å‹3ï¼šå°æ¨¡å‹
--d_model 64 --e_layers 2

# åŠ æƒé›†æˆ
final_pred = 0.5 * pred1 + 0.3 * pred2 + 0.2 * pred3
```

---

### 6ï¸âƒ£ æ”¹è¿›æŸå¤±å‡½æ•°ï¼ˆâ­â­â­ï¼‰

#### A. åŠ æƒMAEæŸå¤±

å¯¹è¿œæœŸé¢„æµ‹ç»™äºˆæ›´é«˜æƒé‡ï¼š

```python
def weighted_mae_loss(pred, target):
    # å¯¹å500æ­¥çš„é¢„æµ‹æƒé‡æ›´é«˜
    weights = torch.linspace(1.0, 2.0, pred.size(1))
    weights = weights.to(pred.device)
    loss = torch.abs(pred - target) * weights
    return loss.mean()
```

#### B. ç»„åˆæŸå¤±

```python
def combined_loss(pred, target):
    mae_loss = F.l1_loss(pred, target)
    mse_loss = F.mse_loss(pred, target)
    # è¶‹åŠ¿æŸå¤±ï¼šé¢„æµ‹å’ŒçœŸå®çš„ä¸€é˜¶å·®åˆ†
    trend_loss = F.l1_loss(pred[:, 1:] - pred[:, :-1], 
                           target[:, 1:] - target[:, :-1])
    return mae_loss + 0.1 * mse_loss + 0.2 * trend_loss
```

#### C. åˆ†ä½æ•°æŸå¤±

é¢„æµ‹å¤šä¸ªåˆ†ä½æ•°ï¼Œæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ï¼š

```python
def quantile_loss(pred, target, quantiles=[0.1, 0.5, 0.9]):
    losses = []
    for i, q in enumerate(quantiles):
        errors = target - pred[:, :, i]
        losses.append(torch.max(q * errors, (q - 1) * errors))
    return torch.cat(losses).mean()
```

---

### 7ï¸âƒ£ ç‰¹å¾å·¥ç¨‹ï¼ˆâ­â­â­ï¼‰

#### A. æ·»åŠ æ—¶é—´ç‰¹å¾

```python
# åœ¨æ•°æ®åŠ è½½æ—¶æ·»åŠ 
features = [
    'day_of_week',      # æ˜ŸæœŸå‡ 
    'day_of_month',     # æœˆä¸­ç¬¬å‡ å¤©
    'week_of_year',     # å¹´ä¸­ç¬¬å‡ å‘¨
    'cumulative_prod',  # ç´¯è®¡äº§é‡
    'prod_rate_change', # äº§é‡å˜åŒ–ç‡
]
```

#### B. æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾

```python
# ä¸ºæ¯ä¸ªè¾“å…¥åºåˆ—è®¡ç®—ç»Ÿè®¡ç‰¹å¾
stats = {
    'mean': seq.mean(),
    'std': seq.std(),
    'min': seq.min(),
    'max': seq.max(),
    'trend': (seq[-100:].mean() - seq[:100].mean()) / len(seq)
}
```

#### C. äº•ç‰¹å¾åµŒå…¥

ä¸ºæ¯å£äº•å­¦ä¹ ç‹¬ç‰¹çš„åµŒå…¥å‘é‡ï¼š

```python
class WellEmbedding(nn.Module):
    def __init__(self, num_wells, d_model):
        super().__init__()
        self.embedding = nn.Embedding(num_wells, d_model)
    
    def forward(self, well_ids, seq):
        well_emb = self.embedding(well_ids)  # [batch, d_model]
        # å°†äº•åµŒå…¥æ·»åŠ åˆ°åºåˆ—ä¸­
        return seq + well_emb.unsqueeze(1)
```

---

### 8ï¸âƒ£ æ•°æ®é¢„å¤„ç†ä¼˜åŒ–ï¼ˆâ­â­â­ï¼‰

#### A. æ›´å¥½çš„å½’ä¸€åŒ–ç­–ç•¥

```python
# å½“å‰ä½¿ç”¨StandardScaler
# å°è¯•ï¼š
from sklearn.preprocessing import RobustScaler  # å¯¹å¼‚å¸¸å€¼æ›´é²æ£’

# æˆ–è€…åˆ†æ®µå½’ä¸€åŒ–
def segment_normalize(data, segment_size=500):
    normalized = []
    for i in range(0, len(data), segment_size):
        segment = data[i:i+segment_size]
        normalized.append((segment - segment.mean()) / segment.std())
    return np.concatenate(normalized)
```

#### B. æ•°æ®å¹³æ»‘

```python
from scipy.signal import savgol_filter

# åœ¨è®­ç»ƒå‰å¹³æ»‘åŸå§‹æ•°æ®
smoothed = savgol_filter(data, window_length=21, polyorder=3)
```

#### C. å¼‚å¸¸å€¼å¤„ç†

```python
# æ£€æµ‹å¹¶ä¿®æ­£å¼‚å¸¸å€¼
def clip_outliers(data, n_std=3):
    mean = data.mean()
    std = data.std()
    lower = mean - n_std * std
    upper = mean + n_std * std
    return np.clip(data, lower, upper)
```

---

### 9ï¸âƒ£ å¤šä»»åŠ¡å­¦ä¹ ï¼ˆâ­â­â­ï¼‰

åŒæ—¶é¢„æµ‹å¤šä¸ªç›¸å…³ä»»åŠ¡ï¼š

```python
class MultiTaskTimeMixer(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.encoder = TimeMixerEncoder(d_model)
        self.production_head = nn.Linear(d_model, 1)
        self.trend_head = nn.Linear(d_model, 1)
        self.decline_head = nn.Linear(d_model, 1)
    
    def forward(self, x):
        features = self.encoder(x)
        prod = self.production_head(features)
        trend = self.trend_head(features)  # é¢„æµ‹è¶‹åŠ¿æ–¹å‘
        decline = self.decline_head(features)  # é¢„æµ‹é€’å‡ç‡
        return prod, trend, decline

# æŸå¤±å‡½æ•°
loss = prod_loss + 0.2 * trend_loss + 0.1 * decline_loss
```

---

### ğŸ”Ÿ è¶…å‚æ•°ä¼˜åŒ–ï¼ˆâ­â­â­ï¼‰

#### A. ä½¿ç”¨Optunaè‡ªåŠ¨æœç´¢

```python
import optuna

def objective(trial):
    # å®šä¹‰æœç´¢ç©ºé—´
    d_model = trial.suggest_categorical('d_model', [64, 128, 192, 256])
    n_heads = trial.suggest_categorical('n_heads', [4, 8, 12, 16])
    dropout = trial.suggest_float('dropout', 0.05, 0.3)
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)
    
    # è®­ç»ƒå¹¶è¿”å›éªŒè¯æŸå¤±
    model = train_model(d_model, n_heads, dropout, lr)
    return model.best_vali_loss

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print('æœ€ä½³å‚æ•°:', study.best_params)
```

#### B. ç½‘æ ¼æœç´¢å…³é”®å‚æ•°

```bash
# æœç´¢æœ€ä½³dropoutå’Œlearning_rate
for dropout in 0.1 0.15 0.2 0.25; do
  for lr in 5e-5 8e-5 1e-4; do
    python scripts/train_sliding_window.py \
      --model_id slide_grid_d${dropout}_lr${lr} \
      --dropout ${dropout} \
      --learning_rate ${lr} \
      --train_epochs 40
  done
done
```

---

## ğŸ¯ æ¨èå®æ–½è·¯çº¿å›¾

### é˜¶æ®µ1ï¼šå¿«é€Ÿæ”¹è¿›ï¼ˆ1-2å¤©ï¼‰

```
1. æ–¹æ¡ˆ1Aï¼šå¾®è°ƒv2æ¨¡å‹ â†’ é¢„æœŸæå‡åˆ°0.35-0.37
2. æ·»åŠ dropoutåˆ°0.15
3. å°è¯•ä¸åŒlearning_rate (5e-5, 8e-5)
```

### é˜¶æ®µ2ï¼šä¸­æœŸä¼˜åŒ–ï¼ˆ3-5å¤©ï¼‰

```
4. å®æ–½æ•°æ®å¢å¼ºï¼ˆå™ªå£°æ³¨å…¥ï¼‰
5. æ·»åŠ L2æ­£åˆ™åŒ–
6. å°è¯•ä¸åŒå­¦ä¹ ç‡ç­–ç•¥ï¼ˆCosineAnnealingï¼‰
```

### é˜¶æ®µ3ï¼šæ·±åº¦ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

```
7. è®­ç»ƒ5æ¨¡å‹é›†æˆ â†’ é¢„æœŸæå‡5-10%
8. æ”¹è¿›æŸå¤±å‡½æ•°ï¼ˆåŠ æƒMAEï¼‰
9. æ·»åŠ ç‰¹å¾å·¥ç¨‹
10. ä½¿ç”¨Optunaè‡ªåŠ¨è°ƒå‚
```

---

## ğŸ“Š é¢„æœŸæ”¹è¿›æ•ˆæœ

| æ–¹æ³• | éš¾åº¦ | é¢„æœŸæå‡ | æ—¶é—´æˆæœ¬ |
|------|------|---------|---------|
| æ–¹æ¡ˆ1Aï¼ˆå¾®è°ƒv2ï¼‰ | ä½ | 5-8% | 0.5å¤© |
| æ•°æ®å¢å¼º | ä¸­ | 3-5% | 1å¤© |
| L2æ­£åˆ™åŒ– | ä½ | 2-3% | 0.5å¤© |
| 5æ¨¡å‹é›†æˆ | ä½ | 5-10% | 2å¤© |
| æ”¹è¿›æŸå¤±å‡½æ•° | ä¸­ | 3-7% | 1å¤© |
| ç‰¹å¾å·¥ç¨‹ | é«˜ | 5-15% | 3å¤© |
| è¶…å‚æ•°ä¼˜åŒ– | ä¸­ | 5-10% | 2å¤© |
| **ç»„åˆæ•ˆæœ** | - | **15-30%** | 1-2å‘¨ |

---

## ğŸ”¥ ç«‹å³è¡ŒåŠ¨å»ºè®®

### ä»Šå¤©å°±å¯ä»¥åšçš„3ä»¶äº‹ï¼š

1. **è¿è¡Œæ–¹æ¡ˆ1A**ï¼ˆæœ€å¿«è§æ•ˆï¼‰
```bash
python scripts/train_sliding_window.py \
  --model_id slide_v2_optimized \
  --input_len 3000 \
  --output_len 1000 \
  --step_len 50 \
  --batch_size 16 \
  --d_model 192 \
  --n_heads 12 \
  --e_layers 4 \
  --d_layers 2 \
  --d_ff 768 \
  --dropout 0.15 \
  --learning_rate 8e-5 \
  --patience 20 \
  --train_epochs 80 \
  --use_gpu
```

2. **å¹¶è¡Œè®­ç»ƒ3ä¸ªä¸åŒéšæœºç§å­çš„æ¨¡å‹**ï¼ˆå‡†å¤‡é›†æˆï¼‰
```bash
# ç»ˆç«¯1
python scripts/train_sliding_window.py --model_id slide_v2_seed1 --input_len 3000 --output_len 1000 --step_len 50 --batch_size 16 --train_epochs 60 --use_gpu

# ç»ˆç«¯2  
python scripts/train_sliding_window.py --model_id slide_v2_seed2 --input_len 3000 --output_len 1000 --step_len 50 --batch_size 16 --train_epochs 60 --use_gpu

# ç»ˆç«¯3
python scripts/train_sliding_window.py --model_id slide_v2_seed3 --input_len 3000 --output_len 1000 --step_len 50 --batch_size 16 --train_epochs 60 --use_gpu
```

3. **ä¿®æ”¹è®­ç»ƒè„šæœ¬æ·»åŠ L2æ­£åˆ™åŒ–**
```python
# ç¼–è¾‘ exp/exp_long_term_forecasting.py
# æ‰¾åˆ° _select_optimizer å‡½æ•°ï¼Œæ·»åŠ  weight_decay=1e-5
```

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **v2å·²ç»å¾ˆå¥½äº†**ï¼šä¸è¦è¿‡åº¦é™ä½æ¨¡å‹å®¹é‡
2. **é›†æˆæ˜¯æœ€ç¨³å¦¥çš„æå‡æ–¹å¼**ï¼š5-10%æå‡å‡ ä¹æ˜¯ä¿è¯çš„
3. **æ•°æ®æ˜¯ç“¶é¢ˆ**ï¼šå¦‚æœäº•æ•°<20ï¼Œå†å¥½çš„æ¨¡å‹ä¹Ÿæœ‰é™
4. **ç¨³å®šæ€§å¾ˆé‡è¦**ï¼šå®æ„¿ç‰ºç‰²1%æ€§èƒ½æ¢å–ç¨³å®šè®­ç»ƒ

---

**ç«‹å³å¼€å§‹æ–¹æ¡ˆ1Aï¼è¿™æ˜¯æŠ•å…¥äº§å‡ºæ¯”æœ€é«˜çš„æ”¹è¿›ï¼** ğŸš€

