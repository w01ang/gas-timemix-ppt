#!/usr/bin/env python3
"""
æ•°æ®å¼‚å¸¸å€¼æ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨NaNã€Infæˆ–å¼‚å¸¸å€¼ï¼Œè¿™äº›å¯èƒ½å¯¼è‡´è®­ç»ƒæ—¶å‡ºç°NaNæŸå¤±
"""

import pandas as pd
import numpy as np
import sys

def check_data(data_path):
    """æ£€æŸ¥æ•°æ®å¼‚å¸¸å€¼"""
    print("=" * 80)
    print("ğŸ” æ•°æ®å¼‚å¸¸å€¼æ£€æŸ¥")
    print("=" * 80)
    print(f"\næ­£åœ¨è¯»å–æ•°æ®: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ•°æ®: {e}")
        return False
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   æ€»è¡Œæ•°: {len(df)}")
    print(f"   åˆ—å: {df.columns.tolist()}")
    
    # æ£€æŸ¥ç›®æ ‡åˆ—
    if 'OT' not in df.columns:
        print("âŒ æœªæ‰¾åˆ°ç›®æ ‡åˆ—'OT'")
        return False
    
    target_col = df['OT']
    
    print("\n" + "=" * 80)
    print("ã€å¼‚å¸¸å€¼æ£€æµ‹ã€‘")
    print("=" * 80)
    
    # 1. æ£€æŸ¥NaN
    nan_count = target_col.isna().sum()
    nan_pct = (nan_count / len(df)) * 100
    print(f"\n1. NaNå€¼:")
    print(f"   æ•°é‡: {nan_count}")
    print(f"   å æ¯”: {nan_pct:.2f}%")
    if nan_count > 0:
        print(f"   âŒ å‘ç°NaNå€¼ï¼")
    else:
        print(f"   âœ… æ— NaNå€¼")
    
    # 2. æ£€æŸ¥Inf
    inf_count = np.isinf(target_col).sum()
    print(f"\n2. Infå€¼:")
    print(f"   æ•°é‡: {inf_count}")
    if inf_count > 0:
        print(f"   âŒ å‘ç°Infå€¼ï¼")
    else:
        print(f"   âœ… æ— Infå€¼")
    
    # 3. åŸºæœ¬ç»Ÿè®¡
    print(f"\n3. æ•°æ®ç»Ÿè®¡:")
    valid_data = target_col.dropna()
    valid_data = valid_data[np.isfinite(valid_data)]
    
    if len(valid_data) == 0:
        print("   âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
        return False
    
    max_val = valid_data.max()
    min_val = valid_data.min()
    mean_val = valid_data.mean()
    median_val = valid_data.median()
    std_val = valid_data.std()
    
    print(f"   æœ€å¤§å€¼: {max_val:.2f}")
    print(f"   æœ€å°å€¼: {min_val:.2f}")
    print(f"   å‡å€¼: {mean_val:.2f}")
    print(f"   ä¸­ä½æ•°: {median_val:.2f}")
    print(f"   æ ‡å‡†å·®: {std_val:.2f}")
    
    # 4. æ£€æŸ¥è´Ÿå€¼
    negative_count = (valid_data < 0).sum()
    print(f"\n4. è´Ÿå€¼:")
    print(f"   æ•°é‡: {negative_count}")
    if negative_count > 0:
        print(f"   âš ï¸  äº§é‡ä¸åº”ä¸ºè´Ÿå€¼")
    else:
        print(f"   âœ… æ— è´Ÿå€¼")
    
    # 5. æ£€æŸ¥å¼‚å¸¸å¤§çš„å€¼
    threshold_high = mean_val + 5 * std_val
    threshold_low = mean_val - 5 * std_val
    outliers_high = (valid_data > threshold_high).sum()
    outliers_low = (valid_data < threshold_low).sum()
    outliers_total = outliers_high + outliers_low
    outliers_pct = (outliers_total / len(valid_data)) * 100
    
    print(f"\n5. å¼‚å¸¸å€¼ (è¶…è¿‡å‡å€¼Â±5å€æ ‡å‡†å·®):")
    print(f"   ä¸Šç•Œ: {threshold_high:.2f}")
    print(f"   ä¸‹ç•Œ: {threshold_low:.2f}")
    print(f"   å¼‚å¸¸é«˜å€¼: {outliers_high}")
    print(f"   å¼‚å¸¸ä½å€¼: {outliers_low}")
    print(f"   æ€»å¼‚å¸¸å€¼: {outliers_total} ({outliers_pct:.2f}%)")
    
    if outliers_pct > 5:
        print(f"   âš ï¸  å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜")
    else:
        print(f"   âœ… å¼‚å¸¸å€¼æ¯”ä¾‹æ­£å¸¸")
    
    # 6. æ£€æŸ¥é›¶å€¼
    zero_count = (valid_data == 0).sum()
    zero_pct = (zero_count / len(valid_data)) * 100
    print(f"\n6. é›¶å€¼:")
    print(f"   æ•°é‡: {zero_count} ({zero_pct:.2f}%)")
    if zero_pct > 10:
        print(f"   âš ï¸  é›¶å€¼æ¯”ä¾‹è¾ƒé«˜")
    
    # 7. æŒ‰äº•æ£€æŸ¥
    if 'äº•å·' in df.columns:
        print(f"\n7. æŒ‰äº•ç»Ÿè®¡:")
        well_count = df['äº•å·'].nunique()
        print(f"   æ€»äº•æ•°: {well_count}")
        
        # æ£€æŸ¥æ¯å£äº•çš„é•¿åº¦
        well_lengths = df.groupby('äº•å·').size()
        print(f"   æœ€é•¿äº•: {well_lengths.max()} æ­¥")
        print(f"   æœ€çŸ­äº•: {well_lengths.min()} æ­¥")
        print(f"   å¹³å‡é•¿åº¦: {well_lengths.mean():.1f} æ­¥")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰äº•å…¨æ˜¯NaN
        wells_with_nan = df.groupby('äº•å·')['OT'].apply(lambda x: x.isna().all()).sum()
        if wells_with_nan > 0:
            print(f"   âŒ æœ‰ {wells_with_nan} å£äº•å…¨æ˜¯NaNå€¼")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ã€æ€»ç»“ã€‘")
    print("=" * 80)
    
    issues = []
    if nan_count > 0:
        issues.append(f"NaNå€¼ ({nan_count})")
    if inf_count > 0:
        issues.append(f"Infå€¼ ({inf_count})")
    if negative_count > 0:
        issues.append(f"è´Ÿå€¼ ({negative_count})")
    if outliers_pct > 5:
        issues.append(f"å¼‚å¸¸å€¼æ¯”ä¾‹é«˜ ({outliers_pct:.1f}%)")
    
    if len(issues) == 0:
        print("\nâœ… æ•°æ®æ­£å¸¸ï¼Œæ— æ˜æ˜¾å¼‚å¸¸")
        print("\nå¦‚æœè®­ç»ƒä»å‡ºç°NaNï¼Œè¯·å°è¯•:")
        print("  1. é™ä½å­¦ä¹ ç‡åˆ° 1e-5")
        print("  2. ä½¿ç”¨æ¢¯åº¦è£å‰ª (å·²è‡ªåŠ¨æ·»åŠ )")
        print("  3. å¢åŠ batch sizeåˆ°32")
        return True
    else:
        print(f"\nâŒ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            print(f"   â€¢ {issue}")
        
        print("\n" + "=" * 80)
        print("ã€å»ºè®®æ¸…ç†æ–¹æ¡ˆã€‘")
        print("=" * 80)
        
        # ç”Ÿæˆæ¸…ç†å»ºè®®
        print("\næ˜¯å¦éœ€è¦æ¸…ç†æ•°æ®? (y/n): ", end='')
        try:
            response = input().strip().lower()
            if response == 'y':
                clean_data(df, data_path)
        except:
            print("\nâš ï¸  è¯·æ‰‹åŠ¨è¿è¡Œæ¸…ç†")
        
        return False

def clean_data(df, original_path):
    """æ¸…ç†å¼‚å¸¸æ•°æ®"""
    print("\nå¼€å§‹æ¸…ç†æ•°æ®...")
    
    df_clean = df.copy()
    original_count = len(df_clean)
    
    # 1. åˆ é™¤NaNå’ŒInf
    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)
    df_clean = df_clean.dropna(subset=['OT'])
    print(f"  åˆ é™¤NaN/Inf: {original_count} â†’ {len(df_clean)} è¡Œ")
    
    # 2. åˆ é™¤è´Ÿå€¼
    df_clean = df_clean[df_clean['OT'] >= 0]
    print(f"  åˆ é™¤è´Ÿå€¼: â†’ {len(df_clean)} è¡Œ")
    
    # 3. è£å‰ªå¼‚å¸¸å€¼
    mean_val = df_clean['OT'].mean()
    std_val = df_clean['OT'].std()
    upper_bound = mean_val + 5 * std_val
    lower_bound = max(0, mean_val - 5 * std_val)
    
    before_clip = len(df_clean)
    df_clean['OT'] = df_clean['OT'].clip(lower_bound, upper_bound)
    print(f"  è£å‰ªå¼‚å¸¸å€¼åˆ° [{lower_bound:.2f}, {upper_bound:.2f}]")
    
    # 4. åˆ é™¤å…¨é›¶çš„äº•
    if 'äº•å·' in df_clean.columns:
        wells_before = df_clean['äº•å·'].nunique()
        well_means = df_clean.groupby('äº•å·')['OT'].mean()
        valid_wells = well_means[well_means > 0].index
        df_clean = df_clean[df_clean['äº•å·'].isin(valid_wells)]
        wells_after = df_clean['äº•å·'].nunique()
        print(f"  åˆ é™¤é›¶å€¼äº•: {wells_before} â†’ {wells_after} å£äº•")
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    output_path = original_path.replace('.csv', '_cleaned.csv')
    df_clean.to_csv(output_path, index=False)
    
    print(f"\nâœ… æ¸…ç†å®Œæˆï¼")
    print(f"   åŸå§‹æ•°æ®: {original_count} è¡Œ")
    print(f"   æ¸…ç†å: {len(df_clean)} è¡Œ (ä¿ç•™ {len(df_clean)/original_count*100:.1f}%)")
    print(f"   ä¿å­˜è·¯å¾„: {output_path}")
    
    print("\nä½¿ç”¨æ¸…ç†åçš„æ•°æ®è®­ç»ƒ:")
    print(f"  --data_path {output_path.split('/')[-1]}")

def main():
    # é»˜è®¤æ•°æ®è·¯å¾„
    default_path = '/Users/wangjr/Documents/yk/timemixer/data/preprocessed_daily_gas_by_well.csv'
    
    if len(sys.argv) > 1:
        data_path = sys.argv[1]
    else:
        data_path = default_path
    
    check_data(data_path)

if __name__ == '__main__':
    main()

