#!/usr/bin/env python3
"""
TimeMixer æ»‘åŠ¨çª—å£è®­ç»ƒè„šæœ¬
ä½¿ç”¨å›ºå®šinput_lenå’Œoutput_lençš„æ»‘åŠ¨çª—å£è¿›è¡Œè®­ç»ƒ
"""

import os
import sys
import argparse
import json
import datetime
from pathlib import Path
import platform
import csv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast
from argparse import Namespace
import torch

class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    def __init__(self, log_dir, model_id):
        self.log_dir = log_dir
        self.model_id = model_id
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.loss_log_path = os.path.join(log_dir, 'training_loss.csv')
        self.summary_log_path = os.path.join(log_dir, 'training_summary.txt')
        
        # åˆå§‹åŒ–CSVæ—¥å¿—
        with open(self.loss_log_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Epoch', 'Train_Loss', 'Vali_Loss', 'Test_Loss', 
                           'Learning_Rate', 'Time'])
        
        # åˆå§‹åŒ–æ–‡æœ¬æ—¥å¿—
        with open(self.summary_log_path, 'w') as f:
            f.write(f"è®­ç»ƒæ—¥å¿— - æ¨¡å‹ID: {model_id}\n")
            f.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
    
    def log_epoch(self, epoch, train_loss, vali_loss, test_loss, lr, epoch_time):
        """è®°å½•æ¯ä¸ªepochçš„æŸå¤±"""
        # å†™å…¥CSV
        with open(self.loss_log_path, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([epoch, f'{train_loss:.7f}', f'{vali_loss:.7f}', 
                           f'{test_loss:.7f}', f'{lr:.10f}', 
                           datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
        
        # å†™å…¥æ–‡æœ¬æ—¥å¿—
        with open(self.summary_log_path, 'a') as f:
            f.write(f"Epoch {epoch}:\n")
            f.write(f"  Train Loss: {train_loss:.7f}\n")
            f.write(f"  Vali Loss:  {vali_loss:.7f}\n")
            f.write(f"  Test Loss:  {test_loss:.7f}\n")
            f.write(f"  Learning Rate: {lr:.10f}\n")
            f.write(f"  Time: {epoch_time:.2f}s\n")
            f.write("-" * 80 + "\n")
    
    def log_final(self, best_epoch, best_vali_loss, total_time, stopped_early=False):
        """è®°å½•æœ€ç»ˆè®­ç»ƒç»“æœ"""
        with open(self.summary_log_path, 'a') as f:
            f.write("\n" + "=" * 80 + "\n")
            f.write("è®­ç»ƒå®Œæˆ\n")
            f.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}s ({total_time/60:.2f}åˆ†é’Ÿ)\n")
            f.write(f"æœ€ä½³Epoch: {best_epoch}\n")
            f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {best_vali_loss:.7f}\n")
            if stopped_early:
                f.write(f"æ—©åœè§¦å‘: æ˜¯\n")
            else:
                f.write(f"æ—©åœè§¦å‘: å¦ï¼ˆå®Œæˆæ‰€æœ‰epochï¼‰\n")
            f.write("=" * 80 + "\n")

def train_sliding_window_model(args, logger=None):
    """è®­ç»ƒæ»‘åŠ¨çª—å£æ¨¡å‹ï¼ˆå¸¦æ—¥å¿—è®°å½•ï¼‰"""
    import time
    import numpy as np
    from utils.tools import EarlyStopping, adjust_learning_rate
    from torch.optim import lr_scheduler
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ»‘åŠ¨çª—å£TimeMixeræ¨¡å‹...")
    print(f"ğŸ“Š é…ç½®å‚æ•°:")
    print(f"   æ¨¡å‹ID: {args.model_id}")
    print(f"   è¾“å…¥é•¿åº¦ (input_len): {args.seq_len}")
    print(f"   è¾“å‡ºé•¿åº¦ (output_len): {args.pred_len}")
    print(f"   æ»‘åŠ¨æ­¥é•¿ (step_len): {args.step_len}")
    print(f"   æ¯”ä¾‹: {args.seq_len}:{args.pred_len} = {args.seq_len/args.pred_len:.2f}:1")
    print(f"   æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"   è®­ç»ƒè½®æ•°: {args.train_epochs}")
    
    if logger:
        print(f"   æ—¥å¿—ä¿å­˜: {logger.log_dir}")
    
    # åˆ›å»ºå®éªŒå¯¹è±¡
    exp = Exp_Long_Term_Forecast(args)
    
    # è·å–æ•°æ®
    train_data, train_loader = exp._get_data(flag='train')
    vali_data, vali_loader = exp._get_data(flag='val')
    test_data, test_loader = exp._get_data(flag='test')
    
    # åˆ›å»ºcheckpointç›®å½•
    path = os.path.join(args.checkpoints, args.model_id)
    if not os.path.exists(path):
        os.makedirs(path)
    
    # è®­ç»ƒè®¾ç½®
    train_steps = len(train_loader)
    early_stopping = EarlyStopping(patience=args.patience, verbose=True)
    model_optim = exp._select_optimizer()
    criterion = exp._select_criterion()
    
    scheduler = lr_scheduler.OneCycleLR(
        optimizer=model_optim,
        steps_per_epoch=train_steps,
        pct_start=args.pct_start,
        epochs=args.train_epochs,
        max_lr=args.learning_rate
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ...")
    training_start_time = time.time()
    
    for epoch in range(args.train_epochs):
        iter_count = 0
        train_loss = []
        exp.model.train()
        epoch_start_time = time.time()
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):
            iter_count += 1
            model_optim.zero_grad()
            
            batch_x = batch_x.float().to(exp.device)
            batch_y = batch_y.float().to(exp.device)
            batch_x_mark = batch_x_mark.float().to(exp.device)
            batch_y_mark = batch_y_mark.float().to(exp.device)
            
            # å‡†å¤‡decoderè¾“å…¥
            if args.down_sampling_layers == 0:
                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()
                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(exp.device)
            else:
                dec_inp = None
            
            # å‰å‘ä¼ æ’­
            outputs = exp.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
            
            # è®¡ç®—æŸå¤±
            f_dim = -1 if args.features == 'MS' else 0
            outputs = outputs[:, -args.pred_len:, f_dim:]
            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(exp.device)
            loss = criterion(outputs, batch_y)
            train_loss.append(loss.item())
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´NaN
            torch.nn.utils.clip_grad_norm_(exp.model.parameters(), max_norm=1.0)
            
            model_optim.step()
            
            # è°ƒæ•´å­¦ä¹ ç‡
            if args.lradj == 'TST':
                adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=False)
                scheduler.step()
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f}")
                speed = (time.time() - epoch_start_time) / iter_count
                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)
                print(f'\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')
        
        # Epochç»“æŸ
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch: {epoch + 1} cost time: {epoch_time:.2f}s")
        
        train_loss = np.average(train_loss)
        vali_loss = exp.vali(vali_data, vali_loader, criterion)
        test_loss = exp.vali(test_data, test_loader, criterion)
        
        current_lr = scheduler.get_last_lr()[0] if args.lradj == 'TST' else model_optim.param_groups[0]['lr']
        
        print(f"Epoch: {epoch + 1}, Steps: {train_steps} | Train Loss: {train_loss:.7f} "
              f"Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f}")
        
        # è®°å½•æ—¥å¿—
        if logger:
            logger.log_epoch(epoch + 1, train_loss, vali_loss, test_loss, current_lr, epoch_time)
        
        # æ—©åœæ£€æŸ¥
        early_stopping(vali_loss, exp.model, path)
        if early_stopping.early_stop:
            print("Early stopping")
            if logger:
                logger.log_final(epoch + 1 - args.patience, early_stopping.val_loss_min, 
                               time.time() - training_start_time, stopped_early=True)
            break
        
        # è°ƒæ•´å­¦ä¹ ç‡
        if args.lradj != 'TST':
            adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=True)
        else:
            print(f'Updating learning rate to {scheduler.get_last_lr()[0]}')
    
    # è®­ç»ƒå®Œæˆ
    total_training_time = time.time() - training_start_time
    
    if logger and not early_stopping.early_stop:
        logger.log_final(args.train_epochs, early_stopping.val_loss_min, 
                        total_training_time, stopped_early=False)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_model_path = path + '/checkpoint.pth'
    exp.model.load_state_dict(torch.load(best_model_path))
    
    print(f"âœ… æ»‘åŠ¨çª—å£æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: checkpoints/{args.model_id}/")
    if logger:
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {logger.log_dir}/")
    
    return exp.model

def main():
    parser = argparse.ArgumentParser(description='TimeMixer æ»‘åŠ¨çª—å£è®­ç»ƒè„šæœ¬')
    
    # å®éªŒæ ‡è¯†
    parser.add_argument('--model_id', type=str, required=True, help='å®éªŒIDï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰')
    parser.add_argument('--comment', type=str, default='sliding_window', help='å®éªŒæ³¨é‡Š')
    parser.add_argument('--description', type=str, default='Sliding window model', help='å®éªŒæè¿°')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--root_path', type=str, default='/Users/wangjr/Documents/yk/timemixer/data', 
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--data_path', type=str, default='preprocessed_daily_gas_by_well.csv', 
                        help='æ•°æ®é›†æ–‡ä»¶å')
    
    # æ ¸å¿ƒçª—å£å‚æ•°
    parser.add_argument('--input_len', type=int, required=True, 
                        help='è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå›ºå®šï¼‰')
    parser.add_argument('--output_len', type=int, required=True, 
                        help='è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆå›ºå®šï¼‰')
    parser.add_argument('--step_len', type=int, default=None, 
                        help='æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆé»˜è®¤=output_lenï¼Œå³æ— é‡å ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=256, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--n_heads', type=int, default=16, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--e_layers', type=int, default=6, help='ç¼–ç å™¨å±‚æ•°')
    parser.add_argument('--d_layers', type=int, default=3, help='è§£ç å™¨å±‚æ•°')
    parser.add_argument('--d_ff', type=int, default=1024, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutæ¯”ä¾‹')
    parser.add_argument('--use_gpu', action='store_true', help='å¯ç”¨GPU/MPSåŠ é€Ÿ')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--train_epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--patience', type=int, default=20, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤æ­¥é•¿
    if args.step_len is None:
        args.step_len = args.output_len
        print(f"â„¹ï¸  æ­¥é•¿æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤å€¼: step_len = output_len = {args.step_len}")
    
    # æ˜¾ç¤ºå‚æ•°ä¿¡æ¯
    print(f"\nğŸ“Š æ»‘åŠ¨çª—å£é…ç½®:")
    print(f"   è¾“å…¥é•¿åº¦ (input_len): {args.input_len}")
    print(f"   è¾“å‡ºé•¿åº¦ (output_len): {args.output_len}")
    print(f"   æ»‘åŠ¨æ­¥é•¿ (step_len): {args.step_len}")
    print(f"   çª—å£æ€»é•¿åº¦: {args.input_len + args.output_len}")
    
    if args.step_len < args.output_len:
        overlap = args.output_len - args.step_len
        overlap_pct = (overlap / args.output_len) * 100
        print(f"   çª—å£é‡å : {overlap} æ­¥ ({overlap_pct:.1f}%)")
    elif args.step_len == args.output_len:
        print(f"   çª—å£é‡å : æ— é‡å ï¼ˆç›¸é‚»çª—å£ï¼‰")
    else:
        gap = args.step_len - args.output_len
        print(f"   çª—å£é—´éš™: {gap} æ­¥")
    
    print(f"   è¾“å…¥:è¾“å‡ºæ¯”ä¾‹ = {args.input_len}:{args.output_len} = {args.input_len/args.output_len:.2f}:1")
    
    # æ„å»ºæ¨¡å‹é…ç½®
    model_args = Namespace(
        task_name='long_term_forecast',
        is_training=1,
        model_id=args.model_id,
        model='TimeMixer',
        data='WELLS',
        root_path=args.root_path,
        data_path=args.data_path,
        features='S',
        target='OT',
        freq='d',
        checkpoints='./checkpoints/',
        seq_len=args.input_len,      # è¾“å…¥é•¿åº¦
        label_len=args.output_len,   # æ ‡ç­¾é•¿åº¦ï¼ˆé€šå¸¸ç­‰äºpred_lenï¼‰
        pred_len=args.output_len,    # é¢„æµ‹é•¿åº¦
        step_len=args.step_len,      # æ»‘åŠ¨æ­¥é•¿ï¼ˆæ–°å¢å‚æ•°ï¼‰
        seasonal_patterns='Monthly',
        inverse=True,
        top_k=5,
        num_kernels=6,
        enc_in=1,
        dec_in=1,
        c_out=1,
        d_model=args.d_model,
        n_heads=args.n_heads,
        e_layers=args.e_layers,
        d_layers=args.d_layers,
        d_ff=args.d_ff,
        moving_avg=49,
        factor=1,
        distil=True,
        dropout=args.dropout,
        embed='timeF',
        activation='gelu',
        output_attention=False,
        channel_independence=1,
        decomp_method='moving_avg',
        use_norm=1,
        down_sampling_layers=1,
        down_sampling_window=2,
        down_sampling_method='avg',
        use_future_temporal_feature=0,
        mask_rate=0.125,
        anomaly_ratio=0.25,
        num_workers=0,
        itr=1,
        train_epochs=args.train_epochs,
        batch_size=args.batch_size,
        patience=args.patience,
        learning_rate=args.learning_rate,
        des=args.description,
        loss='MAE',
        drop_last=True,
        lradj='TST',
        pct_start=0.2,
        use_amp=False,
        comment=args.comment,
        use_gpu=args.use_gpu,
        gpu=0,
        use_multi_gpu=False,
        devices='0,1',
        p_hidden_dims=[128, 128],
        p_hidden_layers=2
    )

    # è‡ªåŠ¨æ£€æµ‹GPUå¯ç”¨æ€§
    if model_args.use_gpu and platform.system() == 'Darwin':
        if not torch.backends.mps.is_available():
            print('âš ï¸  MPSä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU')
            model_args.use_gpu = False
        else:
            print('âœ… ä½¿ç”¨MPSåŠ é€Ÿ')
    elif model_args.use_gpu and torch.cuda.is_available():
        print('âœ… ä½¿ç”¨CUDAåŠ é€Ÿ')
    elif model_args.use_gpu:
        print('âš ï¸  GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU')
        model_args.use_gpu = False
    
    # ä¿å­˜å®éªŒé…ç½®
    config_dir = f"experiments/{args.model_id}"
    os.makedirs(config_dir, exist_ok=True)
    
    config = {
        "model_id": args.model_id,
        "comment": args.comment,
        "description": args.description,
        "input_len": args.input_len,
        "output_len": args.output_len,
        "step_len": args.step_len,
        "window_total_len": args.input_len + args.output_len,
        "input_output_ratio": args.input_len / args.output_len,
        "d_model": args.d_model,
        "n_heads": args.n_heads,
        "e_layers": args.e_layers,
        "d_layers": args.d_layers,
        "d_ff": args.d_ff,
        "train_epochs": args.train_epochs,
        "batch_size": args.batch_size,
        "patience": args.patience,
        "learning_rate": args.learning_rate,
        "use_gpu": model_args.use_gpu,
        "root_path": args.root_path,
        "data_path": args.data_path,
        "created_at": datetime.datetime.now().isoformat()
    }
    
    with open(f"{config_dir}/config.json", 'w') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {config_dir}/config.json")
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    log_dir = f"logs/{args.model_id}"
    logger = TrainingLogger(log_dir, args.model_id)
    print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: {log_dir}/")
    
    # å°†loggerä¼ é€’ç»™model_argsä»¥ä¾¿åœ¨è®­ç»ƒä¸­ä½¿ç”¨
    model_args.logger = logger
    model_args.log_dir = log_dir
    
    # å¼€å§‹è®­ç»ƒ
    train_sliding_window_model(model_args, logger)

if __name__ == "__main__":
    main()

