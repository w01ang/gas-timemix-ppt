#!/usr/bin/env python3
"""
TimeMixer 8:2æ¯”ä¾‹ä¸“ç”¨è®­ç»ƒè„šæœ¬
è¾“å…¥ä¸å®šé•¿æ—¶åºæ•°æ®åºåˆ—ï¼Œè¾“å‡ºå®šé•¿åºåˆ—ç‰‡æ®µï¼ˆè¾“å…¥:è¾“å‡º=8:2ï¼‰
"""

import os
import sys
import argparse
import json
import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast
from argparse import Namespace

def train_8_2_ratio_model(args):
    """è®­ç»ƒ8:2æ¯”ä¾‹æ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ8:2æ¯”ä¾‹TimeMixeræ¨¡å‹...")
    print(f"ğŸ“Š é…ç½®å‚æ•°:")
    print(f"   æ¨¡å‹ID: {args.model_id}")
    print(f"   è¾“å…¥é•¿åº¦: {args.seq_len} (80%)")
    print(f"   è¾“å‡ºé•¿åº¦: {args.pred_len} (20%)")
    print(f"   æ¯”ä¾‹: {args.seq_len}:{args.pred_len} = {args.seq_len/args.pred_len:.1f}:1")
    print(f"   æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"   è®­ç»ƒè½®æ•°: {args.train_epochs}")
    
    # åˆ›å»ºå®éªŒå¯¹è±¡
    exp = Exp_Long_Term_Forecast(args)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ...")
    exp.train(args.model_id)
    
    print(f"âœ… 8:2æ¯”ä¾‹æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: checkpoints/{args.model_id}/")

def main():
    parser = argparse.ArgumentParser(description='TimeMixer 8:2æ¯”ä¾‹ä¸“ç”¨è®­ç»ƒè„šæœ¬')
    
    # å®éªŒæ ‡è¯†
    parser.add_argument('--model_id', type=str, required=True, help='Experiment ID')
    parser.add_argument('--comment', type=str, default='8_2_ratio', help='Experiment comment')
    parser.add_argument('--description', type=str, default='8:2 input-output ratio model', help='Experiment description')
    
    # 8:2æ¯”ä¾‹å‚æ•°
    parser.add_argument('--total_length', type=int, default=1000, help='Total sequence length for 8:2 ratio calculation')
    parser.add_argument('--input_ratio', type=float, default=0.8, help='Input ratio (default: 0.8)')
    parser.add_argument('--output_ratio', type=float, default=0.2, help='Output ratio (default: 0.2)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=256, help='Model dimension')
    parser.add_argument('--n_heads', type=int, default=16, help='Number of attention heads')
    parser.add_argument('--e_layers', type=int, default=6, help='Number of encoder layers')
    parser.add_argument('--d_layers', type=int, default=3, help='Number of decoder layers')
    parser.add_argument('--d_ff', type=int, default=1024, help='Feed-forward dimension')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--train_epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')
    parser.add_argument('--patience', type=int, default=20, help='Early stopping patience')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')
    
    args = parser.parse_args()
    
    # è®¡ç®—8:2æ¯”ä¾‹å‚æ•°
    seq_len = int(args.total_length * args.input_ratio)
    pred_len = int(args.total_length * args.output_ratio)
    
    # éªŒè¯æ¯”ä¾‹
    actual_ratio = seq_len / pred_len
    expected_ratio = args.input_ratio / args.output_ratio
    
    print(f"ğŸ“Š 8:2æ¯”ä¾‹è®¡ç®—:")
    print(f"   æ€»é•¿åº¦: {args.total_length}")
    print(f"   è¾“å…¥æ¯”ä¾‹: {args.input_ratio*100:.0f}%")
    print(f"   è¾“å‡ºæ¯”ä¾‹: {args.output_ratio*100:.0f}%")
    print(f"   è¾“å…¥é•¿åº¦: {seq_len}")
    print(f"   è¾“å‡ºé•¿åº¦: {pred_len}")
    print(f"   å®é™…æ¯”ä¾‹: {actual_ratio:.1f}:1")
    print(f"   æœŸæœ›æ¯”ä¾‹: {expected_ratio:.1f}:1")
    
    if abs(actual_ratio - expected_ratio) > 0.1:
        print(f"âš ï¸  è­¦å‘Š: å®é™…æ¯”ä¾‹ä¸æœŸæœ›æ¯”ä¾‹å·®å¼‚è¾ƒå¤§")
    
    # è®¾ç½®æ¨¡å‹å‚æ•°
    model_args = Namespace(
        task_name='long_term_forecast',
        is_training=1,
        model_id=args.model_id,
        model='TimeMixer',
        data='WELLS',
        root_path='/Users/wangjr/Documents/yk/timemixer/data',
        data_path='preprocessed_daily_gas_by_well.csv',
        features='S',
        target='OT',
        freq='d',
        checkpoints='./checkpoints/',
        seq_len=seq_len,
        label_len=pred_len,  # label_lené€šå¸¸ç­‰äºpred_len
        pred_len=pred_len,
        seasonal_patterns='Monthly',
        inverse=True,
        top_k=5,
        num_kernels=6,
        enc_in=1,
        dec_in=1,
        c_out=1,
        d_model=args.d_model,
        n_heads=args.n_heads,
        e_layers=args.e_layers,
        d_layers=args.d_layers,
        d_ff=args.d_ff,
        moving_avg=49,
        factor=1,
        distil=True,
        dropout=0.1,
        embed='timeF',
        activation='gelu',
        output_attention=False,
        channel_independence=1,
        decomp_method='moving_avg',
        use_norm=1,
        down_sampling_layers=1,
        down_sampling_window=2,
        down_sampling_method='avg',
        use_future_temporal_feature=0,
        mask_rate=0.125,
        anomaly_ratio=0.25,
        num_workers=0,
        itr=1,
        train_epochs=args.train_epochs,
        batch_size=args.batch_size,
        patience=args.patience,
        learning_rate=args.learning_rate,
        des=args.description,
        loss='MSE',
        drop_last=True,
        lradj='TST',
        pct_start=0.2,
        use_amp=False,
        comment=args.comment,
        use_gpu=False,
        gpu=0,
        use_multi_gpu=False,
        devices='0,1',
        p_hidden_dims=[128, 128],
        p_hidden_layers=2
    )
    
    # ä¿å­˜é…ç½®
    config_dir = f"experiments/{args.model_id}"
    os.makedirs(config_dir, exist_ok=True)
    
    config = {
        "model_id": args.model_id,
        "comment": args.comment,
        "description": args.description,
        "total_length": args.total_length,
        "input_ratio": args.input_ratio,
        "output_ratio": args.output_ratio,
        "seq_len": seq_len,
        "pred_len": pred_len,
        "actual_ratio": actual_ratio,
        "d_model": args.d_model,
        "n_heads": args.n_heads,
        "e_layers": args.e_layers,
        "d_layers": args.d_layers,
        "d_ff": args.d_ff,
        "train_epochs": args.train_epochs,
        "batch_size": args.batch_size,
        "patience": args.patience,
        "learning_rate": args.learning_rate,
        "created_at": datetime.datetime.now().isoformat()
    }
    
    with open(f"{config_dir}/config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {config_dir}/config.json")
    
    # å¼€å§‹è®­ç»ƒ
    train_8_2_ratio_model(model_args)

if __name__ == "__main__":
    main()
